{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 13578416,
     "sourceType": "datasetVersion",
     "datasetId": 8622085
    }
   ],
   "dockerImageVersionId": 31154,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "c9b4f37a",
   "cell_type": "code",
   "source": "!pip install ultralytics --upgrade\n\nimport numpy as np\nimport cv2\nimport os\nimport time\nimport yaml\nimport shutil\nfrom ultralytics import YOLO\nimport torch\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Image\n\n# Uncomment if Ultralytics needs installation or updating\n# !pip install ultralytics --upgrade\n\nprint(\"Libraries imported.\")\n\n# Force PyTorch to see all available GPUs (fix for Kaggle T4 x2)\n# Kaggle sometimes hides the second GPU from PyTorch\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\n# Set device (Ensure GPU is enabled in Kaggle settings for fast training)\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {DEVICE}\")\n\n# Check for multiple GPUs\nif torch.cuda.is_available():\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of GPUs available: {num_gpus}\")\n    \n    # List all GPUs\n    for i in range(num_gpus):\n        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n    \n    # For multi-GPU training with YOLO, we'll use device=[0,1]\n    if num_gpus > 1:\n        DEVICE = [0, 1]  # Use both GPUs\n        print(f\"\\nâœ“ Multi-GPU training enabled: Using GPUs {DEVICE}\")\n    else:\n        DEVICE = 0\n        print(f\"\\nâœ“ Single GPU training: Using GPU {DEVICE}\")\n        print(f\"\\nNOTE: If you selected T4 x2 but only see 1 GPU:\")\n        print(\"  1. Make sure you saved the notebook after changing accelerator\")\n        print(\"  2. Restart the kernel (Session â†’ Restart Session)\")\n        print(\"  3. Or use 'Quick Save' button, then reload the page\")\nelse:\n    print(\"No GPU available, using CPU\")\n\n# Define the working directory for outputs\nWORKING_DIR = \"/kaggle/working\"",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:14:11.172139Z",
     "iopub.execute_input": "2025-11-01T15:14:11.172389Z",
     "iopub.status.idle": "2025-11-01T15:15:28.530230Z",
     "shell.execute_reply.started": "2025-11-01T15:14:11.172369Z",
     "shell.execute_reply": "2025-11-01T15:15:28.529520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting ultralytics\n  Downloading ultralytics-8.3.223-py3-none-any.whl.metadata (37 kB)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.3)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.5)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.1.0)\nRequirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.25.0)\nCollecting ultralytics-thop>=2.0.18 (from ultralytics)\n  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nCollecting numpy>=1.23.0 (from ultralytics)\n  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy>=1.23.0->ultralytics)\n  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n  Downloading mkl_fft-2.0.0-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.1 kB)\nINFO: pip is looking at multiple versions of mkl-random to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_random (from numpy>=1.23.0->ultralytics)\n  Downloading mkl_random-1.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nCollecting mkl (from mkl_fft->numpy>=1.23.0->ultralytics)\n  Downloading mkl-2025.3.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nCollecting mkl_random (from numpy>=1.23.0->ultralytics)\n  Downloading mkl_random-1.2.11-22-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nINFO: pip is looking at multiple versions of mkl-umath to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_umath (from numpy>=1.23.0->ultralytics)\n  Downloading mkl_umath-0.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\n  Downloading mkl_umath-0.2.0-21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nDownloading ultralytics-8.3.223-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.2.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.223 ultralytics-thop-2.0.18\nCreating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\nLibraries imported.\nUsing device: cuda\nNumber of GPUs available: 2\n  GPU 0: Tesla T4\n    Memory: 14.7 GB\n  GPU 1: Tesla T4\n    Memory: 14.7 GB\n\nâœ“ Multi-GPU training enabled: Using GPUs [0, 1]\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a5gzl0lhhq",
   "source": "# ==================================================================\n# Fix NumPy 2.x Incompatibility Issue\n# ==================================================================\n# Downgrade numpy to 1.x for compatibility with current matplotlib/ultralytics\n\n!pip install -q \"numpy<2\"\n\nprint(\"âœ“ NumPy downgraded to 1.x for compatibility\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "id": "36n1luxyvl5",
   "cell_type": "code",
   "source": "# ==================================================================\n# 1.5.1 Install Dependencies for Dataset Generation\n# ==================================================================\n\n# Install required packages for dataset generation\nprint(\"Installing dependencies for dataset generation...\")\n\n# Install packages one by one to ensure they're all installed\nimport subprocess\nimport sys\n\npackages = [\n    'aiohttp',\n    'cairosvg',\n    'gitpython',\n    'tqdm',\n    'opencv-python',\n    'pyyaml'\n]\n\nfor package in packages:\n    print(f\"Installing {package}...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n\nprint(\"\\nDataset generation dependencies installed successfully!\")\n\n# Verify imports work\ntry:\n    import aiohttp\n    import cairosvg\n    import git\n    import tqdm\n    import cv2\n    import yaml\n    print(\"âœ“ All dependencies verified and ready to use\")\nexcept ImportError as e:\n    print(f\"âœ— Import error: {e}\")\n    print(\"Please re-run this cell or restart the kernel\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:15:35.556219Z",
     "iopub.execute_input": "2025-11-01T15:15:35.557036Z",
     "iopub.status.idle": "2025-11-01T15:15:55.062973Z",
     "shell.execute_reply.started": "2025-11-01T15:15:35.557011Z",
     "shell.execute_reply": "2025-11-01T15:15:55.062354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Installing dependencies for dataset generation...\nInstalling aiohttp...\nInstalling cairosvg...\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 45.8/45.8 kB 1.8 MB/s eta 0:00:00\n   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 75.6/75.6 kB 5.3 MB/s eta 0:00:00\nInstalling gitpython...\nInstalling tqdm...\nInstalling opencv-python...\nInstalling pyyaml...\n\nDataset generation dependencies installed successfully!\nâœ“ All dependencies verified and ready to use\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "id": "s14a64spbc",
   "cell_type": "code",
   "source": "# Copy scripts from uploaded dataset\n# Update 'chess-watchtower-trainer' to match your dataset name\n!cp -r /kaggle/input/dataset-scripts/* .\n\nprint(\"Current directory:\", os.getcwd())\n!ls -la",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:16:10.697664Z",
     "iopub.execute_input": "2025-11-01T15:16:10.698359Z",
     "iopub.status.idle": "2025-11-01T15:16:10.966369Z",
     "shell.execute_reply.started": "2025-11-01T15:16:10.698335Z",
     "shell.execute_reply": "2025-11-01T15:16:10.965665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Current directory: /kaggle/working\ntotal 52\ndrwxr-xr-x 3 root root  4096 Nov  1 15:16 .\ndrwxr-xr-x 5 root root  4096 Nov  1 15:13 ..\n-rw-r--r-- 1 root root 12994 Nov  1 15:16 gen_board_data.py\n-rw-r--r-- 1 root root 15111 Nov  1 15:16 gen_pieces_data.py\n-rw-r--r-- 1 root root  4475 Nov  1 15:16 get_pieces.py\ndrwxr-xr-x 2 root root  4096 Nov  1 15:13 .virtual_documents\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "id": "b334xut5gt4",
   "cell_type": "code",
   "source": "from pathlib import Path\n\n# Run get_pieces.py\n!python get_pieces.py\n\n# Verify download\nassets_dir = Path('./assets/pieces')\nif assets_dir.exists():\n    piece_count = len(list(assets_dir.rglob('*.png')))\n    print(f\"\\nâœ“ Downloaded {piece_count} piece images\")\nelse:\n    print(\"âš ï¸ Asset download failed!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:19:49.027842Z",
     "iopub.execute_input": "2025-11-01T15:19:49.028131Z",
     "iopub.status.idle": "2025-11-01T15:20:00.876144Z",
     "shell.execute_reply.started": "2025-11-01T15:19:49.028110Z",
     "shell.execute_reply": "2025-11-01T15:20:00.875206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Starting Chess.com downloads...\nDownloading Chess.com pieces:   0%|                     | 0/480 [00:00<?, ?it/s]Cloning Lichess repo (this may take a moment)...\nClone complete. Converting SVGs to PNGs...\nFound 464 SVG files across all piece themes\n\nConverting Lichess SVGs:   0%|                          | 0/464 [00:00<?, ?it/s]\u001b[A\nConverting Lichess SVGs:   3%|â–Œ               | 16/464 [00:00<00:02, 149.51it/s]\u001b[A\nConverting Lichess SVGs:   7%|â–ˆ               | 31/464 [00:00<00:03, 141.85it/s]\u001b[A\nConverting Lichess SVGs:  11%|â–ˆâ–Š              | 51/464 [00:00<00:02, 164.66it/s]\u001b[A\nConverting Lichess SVGs:  15%|â–ˆâ–ˆâ–             | 68/464 [00:00<00:02, 156.60it/s]\u001b[A\nConverting Lichess SVGs:  18%|â–ˆâ–ˆâ–‰             | 84/464 [00:00<00:02, 132.11it/s]\u001b[A\nConverting Lichess SVGs:  23%|â–ˆâ–ˆâ–ˆâ–Œ           | 109/464 [00:00<00:02, 164.58it/s]\u001b[A\nConverting Lichess SVGs:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–          | 133/464 [00:00<00:01, 183.08it/s]\u001b[A\nConverting Lichess SVGs:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 153/464 [00:00<00:01, 179.21it/s]\u001b[A\nConverting Lichess SVGs:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 174/464 [00:01<00:01, 184.84it/s]\u001b[A\nConverting Lichess SVGs:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 193/464 [00:01<00:01, 186.04it/s]\u001b[A\nConverting Lichess SVGs:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 212/464 [00:01<00:01, 175.09it/s]\u001b[A\nConverting Lichess SVGs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 230/464 [00:01<00:01, 173.69it/s]\u001b[A\nConverting Lichess SVGs:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 253/464 [00:01<00:01, 187.21it/s]\u001b[A\nConverting Lichess SVGs:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 273/464 [00:01<00:01, 187.13it/s]\u001b[A\nConverting Lichess SVGs:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 292/464 [00:01<00:00, 176.23it/s]\u001b[A\nConverting Lichess SVGs:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/464 [00:01<00:00, 192.48it/s]\u001b[A\nConverting Lichess SVGs:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 345/464 [00:01<00:00, 219.74it/s]\u001b[A\nConverting Lichess SVGs:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 368/464 [00:02<00:00, 205.66it/s]\u001b[A\nConverting Lichess SVGs:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 391/464 [00:02<00:00, 211.32it/s]\u001b[A\nConverting Lichess SVGs:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 414/464 [00:02<00:00, 216.01it/s]\u001b[A\nConverting Lichess SVGs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 464/464 [00:02<00:00, 186.29it/s]\u001b[A\nLichess conversion complete. Cleaning up repo...\nCleanup complete.\nDownloading Chess.com pieces: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:11<00:00, 42.87it/s]\nChess.com downloads complete: 468/480 successful\nAll assets downloaded to assets/pieces\n\nâœ“ Downloaded 932 piece images\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "id": "y9la0pwoo4",
   "cell_type": "code",
   "source": "from pathlib import Path\n\n# Generate piece detection data (saves clean boards for next step)\n!python gen_pieces_data.py --count 5000 --save-boards\n\n# Verify generation\npieces_data = Path('yolo_pieces')\nboards_data = Path('./assets/boards')\n\nif pieces_data.exists():\n    train_imgs = len(list((pieces_data / 'train' / 'images').glob('*.png')))\n    val_imgs = len(list((pieces_data / 'val' / 'images').glob('*.png')))\n    print(f\"\\nâœ“ Piece detection data: {train_imgs} train, {val_imgs} val\")\n\nif boards_data.exists():\n    board_count = len(list(boards_data.glob('*.png')))\n    print(f\"âœ“ Clean boards saved: {board_count}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:20:02.846360Z",
     "iopub.execute_input": "2025-11-01T15:20:02.846804Z",
     "iopub.status.idle": "2025-11-01T15:23:35.427779Z",
     "shell.execute_reply.started": "2025-11-01T15:20:02.846772Z",
     "shell.execute_reply": "2025-11-01T15:23:35.426716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Generating 5000 positive samples (boards with pieces)\nGenerating 882 negative samples (empty/sparse boards)\nTotal samples: 5882\nFound 38 piece sets: alpha, anarcandy, caliente, california, cardinal...\nWill save clean boards to assets/boards\n\nGenerating samples...\nTrain: 30 piece sets\nVal: 8 piece sets\n\nTrain: 4000 positive + 705 negative = 4705 total\nVal: 1000 positive + 177 negative = 1177 total\nGenerating train positive: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [02:06<00:00, 31.56it/s]\nGenerating train negative: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 705/705 [00:10<00:00, 67.75it/s]\nGenerating val positive: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:31<00:00, 31.67it/s]\nGenerating val negative: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 177/177 [00:02<00:00, 65.54it/s]\nConverting to YOLO format...\n  Train: 4705 images\n  Val: 1177 images\nProcessing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4705/4705 [00:32<00:00, 145.85it/s]\nProcessing val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1177/1177 [00:07<00:00, 149.30it/s]\n\nYOLO dataset created at yolo_pieces\nTrain: 4705 images\nVal: 1177 images\n\nâœ“ Piece detection data: 4705 train, 1177 val\nâœ“ Clean boards saved: 5882\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "id": "xaj9hz7i4ar",
   "cell_type": "code",
   "source": "from pathlib import Path\n\n# Generate board detection data\n!python gen_board_data.py --count 5000\n\n# Verify generation\nboard_data = Path('yolo_board')\n\nif board_data.exists():\n    train_imgs = len(list((board_data / 'train' / 'images').glob('*.png')))\n    val_imgs = len(list((board_data / 'val' / 'images').glob('*.png')))\n    print(f\"\\nâœ“ Board detection data: {train_imgs} train, {val_imgs} val\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:23:43.131618Z",
     "iopub.execute_input": "2025-11-01T15:23:43.132169Z",
     "iopub.status.idle": "2025-11-01T15:27:13.468126Z",
     "shell.execute_reply.started": "2025-11-01T15:23:43.132140Z",
     "shell.execute_reply": "2025-11-01T15:27:13.467381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Generating 500 background images...\nGenerating backgrounds: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:18<00:00, 27.19it/s]\nFound 5882 board images\nFound 500 background images\n\nGenerating 5000 training samples...\nTrain: 4705 boards, 400 backgrounds\nVal: 1177 boards, 100 backgrounds\nGenerating train samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [01:32<00:00, 43.25it/s]\nGenerating val samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:23<00:00, 42.03it/s]\n\nSaving YOLO format...\n  Train: 4000 images\n  Val: 1000 images\nProcessing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:59<00:00, 67.17it/s]\nProcessing val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:15<00:00, 65.71it/s]\n\nYOLO dataset created at yolo_board\nTrain: 4000 images\nVal: 1000 images\n\nâœ“ Board detection data: 4000 train, 1000 val\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "id": "ed0faa80",
   "cell_type": "code",
   "source": "# ==================================================================\n# 4. Training Functions\n# ==================================================================\n\ndef train_model(config_yaml, project_name, base_model='yolov8n.pt', epochs=50, imgsz=640):\n    # Load the specified pretrained model\n    try:\n        model = YOLO(base_model) \n    except Exception as e:\n        print(f\"Error loading base model {base_model}: {e}\")\n        return None\n        \n    print(f\"\\n--- Starting Training for {project_name} using {base_model} ---\")\n    \n    # Determine device info\n    if isinstance(DEVICE, list):\n        print(f\"Multi-GPU Training: Using GPUs {DEVICE}\")\n        device_str = ','.join(map(str, DEVICE))\n    else:\n        device_str = str(DEVICE)\n        if DEVICE == 'cuda' or (isinstance(DEVICE, int) and DEVICE >= 0):\n            print(f\"Single GPU Training: Using {device_str}\")\n        else:\n            print(f\"CPU Training\")\n    \n    # Adjust batch size dynamically based on model size and number of GPUs\n    # With 2 GPUs, we can use larger batch sizes\n    num_gpus = len(DEVICE) if isinstance(DEVICE, list) else 1\n    \n    if base_model == 'yolov8n.pt':\n        batch_size = 32 * num_gpus  # Scale batch size with GPUs\n    else:  # yolov8s or larger\n        batch_size = 16 * num_gpus\n    \n    print(f\"Using batch size: {batch_size} (across {num_gpus} GPU(s))\")\n\n    try:\n        results = model.train(\n            data=os.path.join(WORKING_DIR, config_yaml), \n            epochs=epochs, \n            imgsz=imgsz, \n            device=device_str,  # Pass device as string\n            patience=20, # Stop early if validation stops improving\n            batch=batch_size, \n            augment=True,\n            # Key Augmentations for Chess:\n            perspective=0.0005, # Crucial for simulating camera angles\n            degrees=15.0,\n            translate=0.1,\n            hsv_v=0.5, # Color/brightness augmentation\n            hsv_s=0.7,\n            project=project_name,\n            name='train'\n        )\n    except Exception as e:\n        print(f\"An error occurred during training: {e}\")\n        return None\n        \n    # Save the best model to the working directory for easy download\n    best_model_path = os.path.join(project_name, 'train/weights/best.pt')\n    final_path = os.path.join(WORKING_DIR, f'{project_name}_best.pt')\n    \n    if os.path.exists(best_model_path):\n        shutil.copy(best_model_path, final_path)\n        print(f\"Saved best model to {final_path}\")\n        \n        # Display results graph\n        try:\n            display(Image(filename=os.path.join(project_name, 'train/results.png')))\n        except: pass\n            \n        return final_path\n    \n    print(f\"Best model weights not found at {best_model_path}\")\n    return None",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:27:17.263318Z",
     "iopub.execute_input": "2025-11-01T15:27:17.263978Z",
     "iopub.status.idle": "2025-11-01T15:27:17.272866Z",
     "shell.execute_reply.started": "2025-11-01T15:27:17.263950Z",
     "shell.execute_reply": "2025-11-01T15:27:17.272115Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "id": "6fdaa5cc-2540-4389-8eac-88a3df8648ae",
   "cell_type": "code",
   "source": "# ==================================================================\n# 4.1 Execute Training (Uncomment to Run)\n# ==================================================================\n\n# Model 1: Board Detector using YOLOv8 Nano\nMODEL1_PATH = train_model('yolo_board/data.yaml', 'BoardDetector_Nano', base_model='yolov8n.pt', epochs=50, imgsz=640)\n\n# Model 2: Piece Detector using YOLOv8 Small\nMODEL2_PATH = train_model('yolo_pieces/data.yaml', 'PieceDetector_Small', base_model='yolov8s.pt', epochs=100, imgsz=640)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-11-01T15:27:21.336758Z",
     "iopub.execute_input": "2025-11-01T15:27:21.337040Z",
     "iopub.status.idle": "2025-11-01T15:27:32.414347Z",
     "shell.execute_reply.started": "2025-11-01T15:27:21.337021Z",
     "shell.execute_reply": "2025-11-01T15:27:32.413689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 78.8MB/s 0.1s\n\n--- Starting Training for BoardDetector_Nano using yolov8n.pt ---\nMulti-GPU Training: Using GPUs [0, 1]\nUsing batch size: 64 (across 2 GPU(s))\nUltralytics 8.3.223 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                       CUDA:1 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=64, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/yolo_board/data.yaml, degrees=15.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.5, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0005, plots=True, pose=12.0, pretrained=True, profile=False, project=BoardDetector_Nano, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/BoardDetector_Nano/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 16.2MB/s 0.0s\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \nModel summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n\nTransferred 319/355 items from pretrained weights\n\u001b[34m\u001b[1mDDP:\u001b[0m debug command /usr/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 44363 /root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py\nUltralytics 8.3.223 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                       CUDA:1 (Tesla T4, 15095MiB)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nAttributeErrorAttributeError: : _ARRAY_API not found_ARRAY_API not found\n\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n        data = check_det_dataset(self.args.data)data = check_det_dataset(self.args.data)\n\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n        check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\ncheck_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n       ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n        return f(*args, **kwargs)\nreturn f(*args, **kwargs)\n                     ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n        from matplotlib import font_manager  # scope for faster 'import ultralytics'\nfrom matplotlib import font_manager  # scope for faster 'import ultralytics'\n       ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n        from . import _api, _version, cbook, _docstring, rcsetup\nfrom . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n        from matplotlib.colors import Colormap, is_color_likefrom matplotlib.colors import Colormap, is_color_like\n\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n        from matplotlib import _api, _cm, cbook, scale\nfrom matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n        from matplotlib.ticker import (\nfrom matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n        from matplotlib import transforms as mtransforms\nfrom matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n        from matplotlib._path import (\nfrom matplotlib._path import (\nImportError: ImportErrornumpy.core.multiarray failed to import: \nnumpy.core.multiarray failed to import\n\n\nThe above exception was the direct cause of the following exception:\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py\", line 11, in <module>\n  File \"/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py\", line 11, in <module>\n        trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\ntrainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n                          ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n        super().__init__(cfg, overrides, _callbacks)super().__init__(cfg, overrides, _callbacks)\n\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n        self.data = self.get_dataset()self.data = self.get_dataset()\n\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 651, in get_dataset\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 651, in get_dataset\n        raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\")) from e\nraise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\")) from e\nRuntimeErrorRuntimeError: Dataset '/kaggle/working/yolo_board/data.yaml' error âŒ numpy.core.multiarray failed to import\n: Dataset '/kaggle/working/yolo_board/data.yaml' error âŒ numpy.core.multiarray failed to import\nE1101 15:27:26.767000 428 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 430) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 922, in <module>\n    main()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2025-11-01_15:27:26\n  host      : f684656dec58\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 431)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-11-01_15:27:26\n  host      : f684656dec58\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 430)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "An error occurred during training: Command '['/usr/bin/python3', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '44363', '/root/.config/Ultralytics/DDP/_temp_m4f4ku42140027891938448.py']' returned non-zero exit status 1.\n\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 21.5MB 178.9MB/s 0.1s0.1s<0.0s\n\n--- Starting Training for PieceDetector_Small using yolov8s.pt ---\nMulti-GPU Training: Using GPUs [0, 1]\nUsing batch size: 32 (across 2 GPU(s))\nUltralytics 8.3.223 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                       CUDA:1 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/yolo_pieces/data.yaml, degrees=15.0, deterministic=True, device=0,1, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.5, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0005, plots=True, pose=12.0, pretrained=True, profile=False, project=PieceDetector_Small, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/kaggle/working/PieceDetector_Small/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=12\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2120692  ultralytics.nn.modules.head.Detect           [12, [128, 256, 512]]         \nModel summary: 129 layers, 11,140,244 parameters, 11,140,228 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mDDP:\u001b[0m debug command /usr/bin/python3 -m torch.distributed.run --nproc_per_node 2 --master_port 49125 /root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py\nUltralytics 8.3.223 ğŸš€ Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n                                                       CUDA:1 (Tesla T4, 15095MiB)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nAttributeError: _ARRAY_API not found\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nImportError: numpy.core.multiarray failed to import\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n                ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 651, in get_dataset\n    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\")) from e\nRuntimeError: Dataset '/kaggle/working/yolo_pieces/data.yaml' error âŒ numpy.core.multiarray failed to import\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nAttributeError: _ARRAY_API not found\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 647, in get_dataset\n    data = check_det_dataset(self.args.data)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/data/utils.py\", line 481, in check_det_dataset\n    check_font(\"Arial.ttf\" if is_ascii(data[\"names\"]) else \"Arial.Unicode.ttf\")  # download fonts\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/__init__.py\", line 500, in decorated\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/utils/checks.py\", line 326, in check_font\n    from matplotlib import font_manager  # scope for faster 'import ultralytics'\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nImportError: numpy.core.multiarray failed to import\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py\", line 11, in <module>\n    trainer = DetectionTrainer(cfg=cfg, overrides=overrides)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/detect/train.py\", line 65, in __init__\n    super().__init__(cfg, overrides, _callbacks)\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 160, in __init__\n    self.data = self.get_dataset()\n                ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\", line 651, in get_dataset\n    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\")) from e\nRuntimeError: Dataset '/kaggle/working/yolo_pieces/data.yaml' error âŒ numpy.core.multiarray failed to import\nE1101 15:27:32.085000 434 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 436) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 922, in <module>\n    main()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\n/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2025-11-01_15:27:32\n  host      : f684656dec58\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 437)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-11-01_15:27:32\n  host      : f684656dec58\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 436)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "An error occurred during training: Command '['/usr/bin/python3', '-m', 'torch.distributed.run', '--nproc_per_node', '2', '--master_port', '49125', '/root/.config/Ultralytics/DDP/_temp__doc3_ci140027885499472.py']' returned non-zero exit status 1.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 11
  },
  {
   "id": "28f46650-59d0-4b9a-8d49-1b44351eb104",
   "cell_type": "markdown",
   "source": "## 5. Inference Pipeline\n\nThis section loads the trained models and defines the two-stage inference process, including the crucial post-processing steps of cropping (Stage 1) and coordinate mapping (Stage 2).",
   "metadata": {}
  },
  {
   "id": "5c880b2e-5fed-4869-9801-0e4239ea5789",
   "cell_type": "code",
   "source": "# ==================================================================\n# 5. Inference Pipeline Setup\n# ==================================================================\n\n# Load the trained models.\n# If you ran the training above, the paths are set automatically.\n# If you uploaded pre-trained weights to Kaggle, update the paths manually here:\n\ntry:\n    # Check if variables exist (set during training)\n    _ = MODEL1_PATH\n    _ = MODEL2_PATH\nexcept NameError:\n    # If not, define default paths (e.g., assuming weights are in the working directory)\n    print(\"Model paths not set by training cell. Attempting to use default working directory paths.\")\n    MODEL1_PATH = \"/kaggle/working/BoardDetector_Nano_best.pt\"\n    MODEL2_PATH = \"/kaggle/working/PieceDetector_Small_best.pt\"\n\nmodel_board = None\nmodel_pieces = None\n\ntry:\n    if MODEL1_PATH and os.path.exists(MODEL1_PATH):\n        model_board = YOLO(MODEL1_PATH).to(DEVICE)\n        print(f\"Board model loaded: {MODEL1_PATH}\")\n    else:\n        print(f\"Board model not found at {MODEL1_PATH}. Ensure training was run or path is correct.\")\n        \n    if MODEL2_PATH and os.path.exists(MODEL2_PATH):\n        model_pieces = YOLO(MODEL2_PATH).to(DEVICE)\n        print(f\"Pieces model loaded: {MODEL2_PATH}\")\n    else:\n        print(f\"Pieces model not found at {MODEL2_PATH}. Ensure training was run or path is correct.\")\n\nexcept Exception as e:\n    print(f\"\\nError loading models: {e}\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": 6
  },
  {
   "id": "dd07d8b1-e2ab-48df-921f-e5eb8f62e53c",
   "cell_type": "code",
   "source": "# ==================================================================\n# 5.1 Inference Function\n# ==================================================================\n\ndef run_two_stage_inference(image_path, board_conf=0.7, piece_conf=0.4, use_clahe=False):\n    \"\"\"\n    Implements the fast and accurate two-stage detection pipeline.\n    \"\"\"\n    if model_board is None or model_pieces is None:\n        print(\"Models are not loaded. Cannot run inference.\")\n        return None, 0, None\n\n    start_time = time.time()\n\n    # Load Image using OpenCV\n    img_bgr = cv2.imread(image_path)\n    if img_bgr is None:\n        print(f\"Error loading image: {image_path}\")\n        return None, 0, None\n\n    # STAGE 1: Board Detection (YOLOv8 Nano)\n    # Inference is fast, especially on GPU\n    results_board = model_board(img_bgr, conf=board_conf, iou=0.5, verbose=False)\n\n    if not results_board or len(results_board[0].boxes) == 0:\n        # print(\"No board detected.\")\n        return None, time.time() - start_time, None\n\n    # Post-processing Stage 1: Cropping\n    # Get the highest confidence detection (YOLO sorts them)\n    best_board_box = results_board[0].boxes[0].xyxy[0].cpu().numpy().astype(int)\n    x1, y1, x2, y2 = best_board_box\n\n    # Crop the image\n    img_cropped = img_bgr[y1:y2, x1:x2]\n\n    # STAGE 2: Piece Detection (YOLOv8 Small)\n    # Pre-processing Stage 2: Optional CLAHE\n    if use_clahe:\n        img_cropped = apply_clahe(img_cropped)\n\n    results_pieces = model_pieces(img_cropped, conf=piece_conf, iou=0.4, verbose=False)\n\n    if not results_pieces:\n        return [], time.time() - start_time, best_board_box\n\n    # Post-processing Stage 2: Coordinate Mapping\n    # Crucial Step: Map coordinates from the cropped image back to the original image space.\n    final_detections = []\n    # Access class names dictionary\n    class_names = model_pieces.names\n\n    for box in results_pieces[0].boxes:\n        # Coordinates relative to the cropped image\n        cx1, cy1, cx2, cy2 = box.xyxy[0].cpu().numpy().astype(int)\n        conf = box.conf[0].item()\n        cls_id = int(box.cls[0].item())\n\n        # Map back to original image by adding the top-left corner of the board crop (x1, y1)\n        orig_x1 = cx1 + x1\n        orig_y1 = cy1 + y1\n        orig_x2 = cx2 + x1\n        orig_y2 = cy2 + y1\n\n        final_detections.append({\n            \"class_name\": class_names[cls_id], # FEN Notation\n            \"bbox\": [orig_x1, orig_y1, orig_x2, orig_y2],\n            \"confidence\": conf\n        })\n\n    end_time = time.time()\n    total_time = end_time - start_time\n\n    return final_detections, total_time, best_board_box",
   "metadata": {},
   "outputs": [],
   "execution_count": 7
  },
  {
   "id": "f538c597-6346-4d4c-aa22-ab1766ac282f",
   "cell_type": "markdown",
   "source": "## 6. Visualization and Testing",
   "metadata": {}
  },
  {
   "id": "9cf24acd-cd05-4049-b8bd-683f636f0fc6",
   "cell_type": "code",
   "source": "# ==================================================================\n# 6. Visualization Function\n# ==================================================================\n\ndef visualize_results(image_path, detections, board_box, inference_time):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # Draw board box (Green)\n    if board_box is not None:\n        cv2.rectangle(img, (board_box[0], board_box[1]), (board_box[2], board_box[3]), (0, 255, 0), 4)\n\n    # Draw piece boxes\n    if detections:\n        for det in detections:\n            bbox = det['bbox']\n            label = f\"{det['class_name']} {det['confidence']:.2f}\"\n    \n            # Define colors based on FEN case (Uppercase=White, Lowercase=Black)\n            if det['class_name'].isupper():\n                color = (135, 206, 250) # Light Blue (for White Pieces)\n            else:\n                color = (255, 0, 0) # Red (for Black Pieces)\n    \n            cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)\n            \n            # Add background for text readability\n            (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n            cv2.rectangle(img, (bbox[0], bbox[1] - 20), (bbox[0] + w, bbox[1]), color, -1)\n            text_color = (0,0,0) # Black text\n            cv2.putText(img, label, (bbox[0], bbox[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 2)\n\n    plt.figure(figsize=(12, 12))\n    plt.imshow(img)\n    title_text = f\"Inference Time: {inference_time:.4f} seconds\"\n    if inference_time < 0.5:\n        title_text += \" (Speed Goal Met!)\"\n    plt.title(title_text)\n    plt.axis('off')\n    plt.show()",
   "metadata": {},
   "outputs": [],
   "execution_count": 8
  },
  {
   "id": "9c1fc487-1834-47cf-8ab1-fc393fa4d604",
   "cell_type": "code",
   "source": "# ==================================================================\n# 6.1 Example Execution (Test the pipeline)\n# ==================================================================\n\n# --- Example Execution ---\n# Update the TEST_IMAGE_PATH to point to a specific validation image in your dataset\n\n# try:\n    # # Example path (you must change this to a real image file in your validation set)\n    # # It's often useful to test on an image from the board validation set.\n    # TEST_IMAGE_PATH = f\"{BOARD_DATA_PATH}/val/images/example_test_image.jpg\"\n\n    # # Run inference (We recommend trying use_clahe=True to see if it improves results)\n    # if os.path.exists(TEST_IMAGE_PATH):\n    #     detections, inference_time, board_box = run_two_stage_inference(TEST_IMAGE_PATH, use_clahe=True)\n    #\n    #     if detections is not None:\n    #         visualize_results(TEST_IMAGE_PATH, detections, board_box, inference_time)\n    # else:\n    #      print(f\"Test image not found at: {TEST_IMAGE_PATH}. Please update the path.\")\n# except NameError:\n    # print(\"Skipping test execution because data paths (like BOARD_DATA_PATH) might not be properly defined yet.\")\n",
   "metadata": {},
   "outputs": [],
   "execution_count": 9
  }
 ]
}